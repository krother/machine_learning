{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import plot_roc_curve, auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Penguins again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved everything up to the feature engineering to a separate module\n",
    "from penguin_dataset import preprocess_penguins\n",
    "\n",
    "Xtrain, ytrain, Xval, yval, Xtest, ytest = preprocess_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open('../models/penguin_forest.pkl', 'rb'))\n",
    "\n",
    "ypred = m.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame(m.predict_proba(Xtest), columns=['Ade', 'Chin', 'Gen'])\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(ytest, ypred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf,\n",
    "            xticklabels = np.unique(ytest),\n",
    "            yticklabels = np.unique(ytest),\n",
    "            cmap = 'Blues',\n",
    "            annot=True,\n",
    "            fmt='g'\n",
    "            )\n",
    "\n",
    "# fmt is used to switch off scientific notation\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "* The percentage of correct guesses\n",
    "* 0.0 worst - 1.0 best\n",
    "* simple way of assessing the model\n",
    "* fails with imbalanced classes\n",
    "\n",
    "$acc = \\dfrac{TP+TN}{TP+TN+FP+FN}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytest, ypred) #compare predictive results to actual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "* High Precision = more relevant than irrelevant results returned (at the expense of missing some relevant ones)\n",
    "* Also called False Positive Rate\n",
    "* High when false positives are low\n",
    "\n",
    "$precision = \\dfrac{TP}{TP+FP}$\n",
    "    \n",
    "### Recall\n",
    "    * High Recall = most of the relevant results returned, (at the expense of including bad results)\n",
    "    * Also called True Positive Rate\n",
    "    * High when false negatives are low \n",
    "    * Rec = TP/ (TP+FN)\n",
    "\n",
    "* Remember the two scenarios! Which one suits which measure?\n",
    "\n",
    "### F1 Score \n",
    "\n",
    "$F1 = \\dfrac{precision \\cdot recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these only works for binary classification\n",
    "# precision_score(ytest, ypred)\n",
    "# recall_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred, zero_division='warn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operator Characteristic\n",
    "\n",
    "**ROC plots True Positive Rate (TPR, y axis) against False Positive Rate (FPR, x axis).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain a binary classification\n",
    "ytrain_adelie = ytrain == 'Adelie'\n",
    "ytest_adelie = ytest == 'Adelie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(Xtrain, ytrain_adelie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=m.predict_proba(Xtest) \n",
    "fpr, tpr, threshold = roc_curve(ytest_adelie, probs[:,0]);\n",
    "\n",
    "plot_roc_curve(m, Xtest, ytest_adelie)  \n",
    "plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "\n",
    "plt.ylabel(\"TPR, Recall\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim([0,1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > 0.7 acceptable. Better > 0.8\n",
    "auc_score = auc(tpr, fpr)\n",
    "print(f\"Area under the curve: {auc_score:5.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

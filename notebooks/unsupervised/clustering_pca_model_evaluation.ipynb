{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning + Model Evaluation\n",
    "\n",
    "**Goal: predict the Body Mass from the other columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/penguins_simple.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, random_state=777)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "* clusters are spherical\n",
    "* all clusters have the same size\n",
    "* every cluster has a center point\n",
    "* you set the number of clusters before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='error', drop='first'), ['Species', 'Sex']),\n",
    "    ('scale', MinMaxScaler(), ['Culmen Length (mm)', 'Culmen Depth (mm)',\n",
    "       'Flipper Length (mm)', 'Body Mass (g)'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.fit(X)\n",
    "Xt = col.transform(X)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "km.fit(Xt)\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.copy()\n",
    "train['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='Culmen Length (mm)', y='Body Mass (g)', hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue='cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveat:\n",
    "\n",
    "* clustering with Euclidean distance does not work well with many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "* silhouette score: describes the overall shape of the cluster (penalizes large surface)\n",
    "* compare to a reference set (e.g. Species)\n",
    "* calculate clusters for different hyperparameters and compare some metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering\n",
    "\n",
    "* you get the number of clusters as a result\n",
    "* two hyperparameters: \n",
    "  * minimum # points belonging to a cluster\n",
    "  * maximum distance for two points in the same cluster (eps)\n",
    "* finds outliers (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBSCAN(eps=0.13, min_samples=5, metric='euclidean')\n",
    "m.fit(Xt)\n",
    "train['dbscan'] = m.fit_predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue='dbscan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics:\n",
    "\n",
    "* euclidean : works well only <10 features\n",
    "* manhattan : works well only <10 features\n",
    "* cosine similarity : angle between two vectors, good for large number of features\n",
    "* Jaccard distance / Tanimoto score : for large numbers of binary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we measure statistical dependence of features with correlation coefficients\n",
    "# 1.0 = identical, 0.0 = independent\n",
    "sns.heatmap(pd.DataFrame(Xt, columns=['sp1', 'sp2', 'sex', 'beak_len', 'beak_wid', 'flipper', 'mass']).corr().round(2), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to have a mean of zero for PCA\n",
    "sc = StandardScaler()\n",
    "Xs = sc.fit_transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA performs a linear transformation. All the features get transformed into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output data of the PCA are *orthogonal* or *independent* features\n",
    "# --> super important for linear models\n",
    "sns.heatmap(pd.DataFrame(Xp).corr().round(2), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output features of the PCA are *ranked* : the first feature is the most important one, the second feature is the second is the second most important etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use only the most representative features -> Dimensionality Reduction\n",
    "# --> models can be trained faster\n",
    "# --> we avoid overfitting\n",
    "\n",
    "## Disadvantage: we don't know what the new features after PCA mean (no labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many components to use?\n",
    "pd.Series(pca.explained_variance_ratio_).plot()\n",
    "# first new feature explains 50% of the variance in the data\n",
    "# second new feature explains 25% of the variance\n",
    "# features 3-6 only explain noise, we can remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to use PCA for?\n",
    "\n",
    "* use the output as an input for further modeling (as a preprocessing step)\n",
    "* use the output for plotting / clustering to explore the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pd.DataFrame(Xp), x=0, y=1, hue=train['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ColumnTransformer([\n",
    "    # check Andreas MÃ¼llers scikit videos on this\n",
    "    # for how to write your own preprocessors\n",
    "    ('onehot', OneHotEncoder(handle_unknown='error', drop='first'), ['Species', 'Sex']),\n",
    "    ('scale', 'passthrough', ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    col,\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=4),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train.iloc[:, :-2]\n",
    "ytrain = train['Body Mass (g)']\n",
    "\n",
    "pipeline.fit(Xtrain, ytrain)\n",
    "ypred_train = pipeline.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to debug/inspect models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. validation score (should be easy with a pipeline)\n",
    "Xval = test\n",
    "yval = test['Body Mass (g)']\n",
    "\n",
    "## NEVER FIT ANYTHING ON val/test DATA!!! pipeline.fit(Xval, yval)\n",
    "ypred_val = pipeline.predict(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(ytrain, ypred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(yval, ypred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(ytrain, ypred_train).round() # g in penguin weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(yval, ypred_val).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. cross-validation\n",
    "#    good rule of thumb, 5x training time\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(pipeline, Xtrain, ytrain,\n",
    "                    cv=5,\n",
    "                    scoring='neg_mean_absolute_error',\n",
    "                    return_train_score=True\n",
    ")\n",
    "pd.DataFrame(cv).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for:\n",
    "# - big differences between test/train scores in the same row (e.g. overfitting)\n",
    "# - lots of variation in the same column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. examine residuals (errors)\n",
    "#    (in classification: inspect some misclassified points)\n",
    "residual = ytrain - ypred_train\n",
    "residual.hist(bins=20)\n",
    "# what are the biggest/smallest errors?\n",
    "# is it a gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for dependence of the errors with some feature (e.g. time in a time series)\n",
    "train['residual'] = residual\n",
    "sns.scatterplot(data=train, y='residual', x='Body Mass (g)')\n",
    "# are the residuals distributed evenly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. in linear regression, check other assumptions\n",
    "#    (includes some statistical tests and more plots)\n",
    "#    in time series you *must* check for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. bootstrapping\n",
    "#    (resamples the dataset 100-1000 times) -> takes much more training time\n",
    "#    gives you a reliable estimate of your training/test score with confidence interval\n",
    "#    -> do this at the very end of a training with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. inspect the influence of different features\n",
    "#    linear regression: check coefficients directly\n",
    "#    better output with statsmodels (p-values and confidence intervals for each coefficient)\n",
    "pipeline.named_steps['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"1 cm of beak length means 385 g of penguin weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    random forest / boosting\n",
    "#    - m.feature_importance_\n",
    "#    - Shapley values (in catboost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "We want to identify clusters/groups/classes in the data. Most of the time you would want to do that as part of an exploratory analysis.\n",
    "\n",
    "It is more of a trial and error approach than a problem with a clearly defined solution. After all it is often used in situations were we do not have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-Means Clustering\n",
    "\n",
    "0. Scale the data!\n",
    "1. Initial centroids (randomly or from the data)\n",
    "2. Assign datapoints to nearest centroid\n",
    "3. calculate mean of all points off a cluster and set this as the new centroid\n",
    "4. repeat until nothing changes anymore.\n",
    "\n",
    "\n",
    "Visualization: https://stanford.edu/class/engr108/visualizations/kmeans/kmeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# For big datasets: MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with different clusters\n",
    "X, y = make_blobs(n_features=2, n_samples=200, centers=5, random_state=42, cluster_std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(X[:,0].astype(float), X[:, 1].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data; MinMaxScaler scales data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean = KMeans(n_clusters=5)\n",
    "Kmean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and draw the centroids of the clusters\n",
    "plt.scatter(X[:,0].astype(float), X[:, 1].astype(float))\n",
    "plt.scatter(Kmean.cluster_centers_[:,0],Kmean.cluster_centers_[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with labels\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(X[:,0].astype(float), X[:, 1].astype(float), c=Kmean.labels_)\n",
    "legend1 = ax.legend(*scatter.legend_elements())\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting an unseen point: \n",
    "test = np.array([0.5, 0.7]).reshape(1, -1)\n",
    "Kmean.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluation\n",
    "\n",
    "#### 2.1 Inertia\n",
    "\n",
    "Sum of squared distances of samples to their closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Inertia + Elbow-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the inertia for different numbers of clusters\n",
    "inertia = []\n",
    "for i in range(1,20):\n",
    "    Kmean = KMeans(n_clusters=i)\n",
    "    Kmean.fit(X)\n",
    "    inertia.append(Kmean.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the inertia\n",
    "plt.plot(inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Silhouette Score\n",
    "\n",
    "$$\n",
    "\\frac{b-a}{max(a, b)}\n",
    "$$\n",
    "\n",
    "where a is the mean intra-cluster distance and b is the mean nearest-cluster distance. It takes on values between -1 and 1, where 1 is the best: clusters are dense and well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean = KMeans(n_clusters=4)\n",
    "Kmean.fit(X)\n",
    "print(f'The silhouette_score is {round(silhouette_score(X, Kmean.labels_), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of K-means: \n",
    "    - fast\n",
    "    - easy to understand\n",
    "    \n",
    "#### Disadvantages: \n",
    "    - non-deterministic\n",
    "    - only works well for clusters of convex shape and similar size\n",
    "    - we have to specify k\n",
    "    - sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality: \n",
    "- euclidean distance works bad for high dimensional spaces (many features)\n",
    "- all data points appear to be further from each other (increasing distance) the more dimensions we have, and with little distance variation -> bad for clustering. \n",
    "\n",
    "What is a possible solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. When KMeans fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "X_moons, _ = make_moons(1000, noise=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_moons = scaler.fit_transform(X_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the data using Kmeans\n",
    "Kmean_moons = KMeans(n_clusters=2)\n",
    "Kmean_moons.fit(X_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=Kmean_moons.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DBSCAN - Density based clustering\n",
    "\n",
    "- creates clusters based on density -> clusters can have any shape\n",
    "- no assumption about number of cluster\n",
    "- hyperparamters: \n",
    "    - How close do points have to be together for a cluster (eps)\n",
    "    - How many points constitute a cluster (min_samples)\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "- Choose a random datapoint from the dataset\n",
    "- Calculate the distance to each datapoint\n",
    "- If there are are more then min_samples - 1 datapoints closer than eps from the datapoint, open a cluster and go to the next datapoint within the cluster; if not, mark the datapoint as outlier and go to the next datapoint\n",
    "- Do the same for the next datapoint until all datapoints were visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "dbscan = DBSCAN(eps=0.04, min_samples=5)\n",
    "labels = dbscan.fit_predict(X_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "plt.scatter(X_moons[:,0].astype(float), X_moons[:, 1].astype(float), c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X_moons, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 How to choose epsilon?\n",
    "\n",
    "We could check out the typical closest distance between two datapoints in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the typical distance\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X_moons)\n",
    "distances, indices = nbrs.kneighbors(X_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the distances\n",
    "distances = np.sort(distances, axis=0)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distances\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages: \n",
    "- clusters can have any shape and size\n",
    "- outliers are treated as such\n",
    "- we don't have to specify number of clusters beforehand\n",
    "\n",
    "### Disadvantages: \n",
    "- slower\n",
    "- harder to tune\n",
    "- harder to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Hierarchical Clustering (Agglomerative Clustering)\n",
    "Builds a hierarchy from the bottom up. \n",
    "\n",
    "- Each datapoint starts as their own little cluster. \n",
    "- Then, a hierarchy is build through always aggregating the points that are closest together. \n",
    "- In the end, we have one big cluster, but can \"cut\" the tree of at each level to get specific number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "X_2, _ = make_blobs(n_features=2, n_samples=200,centers=4, random_state=42, cluster_std=1.5)\n",
    "agg = hierarchy.linkage(X_2)\n",
    "d = hierarchy.dendrogram(agg, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2[:,0], X_2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "*In my opinion, Linear Regression is the most important Machine Learning model that exists. It is worth to study well.*\n",
    "\n",
    "* predicting continous variables is commonplace in data science\n",
    "* the results are interpretable\n",
    "* can solve complex problems with proper engineering of features\n",
    "* Linear Regression has been backed up by deep statistical research\n",
    "* Linear Regression is the base to understand other linear models (such as Logistic Regression or Poisson Regression)\n",
    "* Prerequisite for understanding neural networks\n",
    "* very often does the job sufficiently well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Business Goal\n",
    "\n",
    "**We predict prices of houses from a multitude of features.**\n",
    "\n",
    "A detailed description of the challenge can be found on [Kaggle.com](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/housing/housing_train.csv', index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split Data into Training, Validation and Test sets\n",
    "The test set has been already sliced off by Kaggle. They only give us the input data but not the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['LotFrontage', 'OverallQual', 'YearBuilt']\n",
    "train[COLUMNS].isna().sum().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train[COLUMNS + ['SalePrice']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a matrix of input features\n",
    "Xtrain = train[COLUMNS]\n",
    "Xval = val[COLUMNS]\n",
    "\n",
    "# y is a vector of scalar values --> Regression\n",
    "ytrain = train['SalePrice']\n",
    "yval = val['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval.shape, yval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch up the missing data\n",
    "avg = Xtrain.mean()\n",
    "Xtrain = Xtrain.fillna(avg)\n",
    "Xval = Xval.fillna(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train a Linear Regression Model\n",
    "\n",
    "The model is:\n",
    "\n",
    "$\\hat y = w_1x_1 + w_2x_2 + .. + w_nx_n + w_0$\n",
    "\n",
    "There are two ways to solve a Linear Regression Model\n",
    "\n",
    "#### a) Normal Equation\n",
    "\n",
    "Uses an analytical approach to calculate coefficients directly.\n",
    "This is a closed-form solution called the **Normal Equation**\n",
    "\n",
    "The Normal Equation has two big disadvantages:\n",
    "\n",
    "* quadratic time complexity $O(N^2)$\n",
    "* it can gets stuck if your features are redundant\n",
    "\n",
    "Usually, b) is the better choice.\n",
    "\n",
    "#### b) Gradient Descent\n",
    "\n",
    "Iteratively optimizes the coefficients to find the lowest possible MSE.\n",
    "\n",
    "* always finds the minimum (MSE is a convex function)\n",
    "* partial derivative (linear time complexity to data points and features)\n",
    "\n",
    "This is the implementation used in practically all common libraries (scikit, statsmodels, R, Spark, TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LinearRegression(fit_intercept=True)\n",
    "m.fit(Xtrain, ytrain)\n",
    "m.score(Xtrain, ytrain)  # R^2 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate the Model\n",
    "\n",
    "#### R squared\n",
    "\n",
    "* 0 = no explainability from the model's correlation\n",
    "* 1 = the model completely explains the variance in the data\n",
    "\n",
    "#### MSE\n",
    "\n",
    "* Mean Squared Error\n",
    "* tt is very sensitve to outliers - each residual is squared, so\n",
    "* residuals greater than one have a disproportionate big effect on outliers\n",
    "* residuals less than one have a disproportionate small effect on outliers\n",
    "\n",
    "#### MAE\n",
    "\n",
    "* Mean Absolute Error\n",
    "* average of the absolute residuals\n",
    "* less sensitive to outliers than the MSE\n",
    "* same unit as the target variable\n",
    "\n",
    "#### RMSL\n",
    "\n",
    "* Root Mean Squared Log Error\n",
    "* doesn't penalise over-estimates as much as underestimates\n",
    "* good for count data that stretches over several orders of magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = m.predict(Xtrain)\n",
    "mse_train = mean_squared_error(ytrain, ypred)\n",
    "mae_train = mean_absolute_error(ytrain, ypred)\n",
    "\n",
    "print(f\"training MSE {mse_train:4.2f}\")\n",
    "print(f\"training MAE {mae_train:4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_val = m.predict(Xval)\n",
    "mse_val = mean_squared_error(yval, ypred_val)\n",
    "mae_val = mean_absolute_error(yval, ypred_val)\n",
    "print(f\"validation MSE {mse_val:4.2f}\")\n",
    "print(f\"validation MAE {mae_val:4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the coefficients\n",
    "\n",
    "# LotFrontage, OverallQual, YearBuilt\n",
    "m.coef_.round(1), m.intercept_.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deploy the model\n",
    "here we just save the model to a file to use it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(m, open('../models/linreg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Prediction for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LotFrontage, OverallQual, YearBuilt\n",
    "house = [[50, 10, 1975]]\n",
    "m.predict(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model with Statsmodels\n",
    "\n",
    "**Statsmodels** adds more interpretability to the model. The interface does not fit 100% to scikit though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain['intercept'] = 1  # <-- OLS does not do this on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = OLS(ytrain, Xtrain)  # opposite order\n",
    "result = sm.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "\n",
    "Linear Regression describes a relationship between two or more variables that tries to find a hyperplane through the data that minimizes the MSE.\n",
    "\n",
    "We model an output variable y as a linear combination of input variables X.\n",
    "\n",
    "**Prediction:**\n",
    "\n",
    "$\\hat{y} = \\hat{w}X$ or\n",
    "\n",
    "$\\hat{y} = \\hat{w_1} * x_1 + \\hat{w_2} * x_2 + ... + \\hat{w_n} * x_n + \\hat{w_0}$\n",
    "\n",
    "\n",
    "**True Relationship:**\n",
    "\n",
    "$y = Xw + \\epsilon$\n",
    "\n",
    "where\n",
    "\n",
    "- X ($x_1, x_2, x_3, ..., x_n$) are the input features, e.g. size of the house, quality, year it was built\n",
    "- $\\hat{y}$ is our prediction for the outcome value (cnt; count of rentals)\n",
    "- $\\hat{w}$ $(w_0, w_1, ..., w_n)$ are the estimated coefficients for our input features\n",
    "- $\\epsilon$ is an error term (some unexplainable randomness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 + 3 + 1 Assumptions\n",
    "\n",
    "There are several assumptions that you will want to check before you use a LinReg model in production.\n",
    "\n",
    "* **The first 4 assumptions guarantee that the best possible MSE is found** (by the Gauss-Markov Theorem). If they hold, you can use the model to make predictions.\n",
    "* **The next 3 assumptions provide a basis for explainability**. If they hold, you can interpret the p-values of coefficients.\n",
    "* **The last assumption (autocorrelation) is important for time series data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1) The true relationship between X and y is linear in the parameters\n",
    "\n",
    "The dependent variable, y, is linearly depending on the independent variable(s), x, and the error, $\\epsilon$ as \n",
    "\n",
    "$y = w_0 + w_1 * x + \\epsilon$\n",
    "\n",
    "To check this assumption, look at the scatterplot of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2) The sample is a random sample\n",
    "\n",
    "The training data has to be representative for the whole population, otherwise the model **fails to generalize**.\n",
    "This assumption mainly depends on your data collection/cleaning. It is difficult to check at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3) There is variation in the X variables\n",
    "\n",
    "The X variable(s) take(s) on different values. Otherwise no information can be gained by looking at X.\n",
    "\n",
    "To check this assumption, plot the histograms of your features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4) Zero Conditional Mean\n",
    "\n",
    "The mean of the error term $\\epsilon$ conditional on X is 0.\n",
    "There is no relationship between X and the error term $\\epsilon$.\n",
    "\n",
    "$E(\\epsilon|X) = 0$\n",
    "\n",
    "To check this assumption, plot the **residuals**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(Xtrain['LotFrontage'], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(Xtrain['YearBuilt'], ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If these assumptions hold, the model is unbiased**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5) Homoskedasticity\n",
    "\n",
    "Homoskedasticity means that the variance of the y does not change over time. \n",
    "\n",
    "#### Check #1: Plot the residuals against X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ypred - ytrain\n",
    "plt.plot(Xtrain['YearBuilt'], residuals, 'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check #2: Breusch-Pagan Test\n",
    "\n",
    "What the Breusch-Pagan-Test does intuitively:\n",
    "\n",
    "- Runs a linear regression of y on X\n",
    "- It calculates the residuals\n",
    "- It runs a linear regression of the residuals on X $\\hat{\\epsilon} = \\delta_0 + \\delta_1 * x_1$\n",
    "- It uses the $R^2$ value to determine whether the explanatory variables are able to explain the residuals; If that is the case, then we have some kind of heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "het_breuschpagan(residuals, Xtrain)\n",
    "# The fourth value is important: it gives you the p-value\n",
    "# The Null hypothesis is that there is no heteroskedasticity\n",
    "# In this case we reject the null hypothesis because the p-value is very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A6) Normal Distribution of the Error Terms\n",
    "\n",
    "This assumption is convenient if we want to make precise statements about effect sizes (the w coefficients of the linear regression).\n",
    "\n",
    "Let's look at statsmodels output again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better: QQ-plot\n",
    "from scipy.stats import probplot\n",
    "\n",
    "probplot(residuals, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jarque-Bera-Test\n",
    "from scipy.stats import jarque_bera\n",
    "\n",
    "jarque_bera(residuals)\n",
    "# second value is the p-value\n",
    "# if we fail to reject the Null Hypothesis (p > 0.05)\n",
    "# we can confidently say that the residuals are probably normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if normality fails to hold?\n",
    "\n",
    "If the sample size is big and we have many features, this is not a problem. \n",
    "We can use the Central Limit Theorem and still apply the statistical evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A7) No Multicolinearity\n",
    "\n",
    "If this assumption fails, we still don't have any problems to make predictions. Statements about the coefficients become very difficult if we have multicolinearity.\n",
    "\n",
    "What is Multicolinearity?<br>\n",
    "Two features are perfectly colinear, if one is a linear transformation of the other. For example height in cm and height in m would be perfectly colinear.\n",
    "\n",
    "There will always be correlation between different variables. So when do we consider it to be a problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance inflation factors\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "Xt = add_constant(Xtrain)\n",
    "pd.Series([variance_inflation_factor(Xtrain.values, i)\n",
    "           for i in range(Xtrain.shape[1])],\n",
    "          index=Xtrain.columns)\n",
    "\n",
    "# A VIF greater than 5 indicates high multicolinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

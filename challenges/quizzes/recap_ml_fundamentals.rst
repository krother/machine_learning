Recap Questions
===============

Machine Learning Fundamentals
-----------------------------

1. How would you define Machine Learning?
2. Name 4 common supervised learning tasks.
3. Name 4 methods for supervised and unsupervised learning.
4. What is overfitting?
5. How to measure overfitting and underfitting?

Gradient Descent
----------------

1. Which functions are GD algorithms minimizing?
2. What is the gradient at a local minimum?
3. What is a convex function?
4. Why would one use Stochastic Gradient Descent?
5. How would you notice that Gradient Descent in scikit is not converging, and what might be reasons?


Linear Regression
-----------------

1. Can you use Linear Regression with millions of data points or features?
2. What are pros and cons of Gradient Descent versus the Normal Equation?
3. Does it help to scale the data in Linear Regression?
4. What assumptions does Linear Regression require?
5. What is multicolinearity?

Hyperparameter Optimization
---------------------------

1. What is the difference between a model parameter and a hyperparameter?
2. How does GridSearch work?
3. Do feature engineering methods have hyperparameters?
4. Why shouldnâ€™t you tune hyperparameters using the test set?
5. How can you decide what range of hyperparameters to try?

Bias and Variance
-----------------

1. What is the curse of dimensionality?
2. Give an example of high bias
3. Explain what the bias-variance tradeoff is
4. What is the difference between L1 and L2 regularization?
5. Describe strengths and weaknesses of Random Forest models

Three Things
------------

* Name 3 types of trainable neural network layers
* Name 3 assumptions of Linear Regressions and how to test them.
* Name 3 loss functions
* Name 3 things that help against overfitting.

Data Wrangling
--------------

* How can you fill missing values?
* Explain 2 different ways to normalize data.
* What advantages does PCA have?

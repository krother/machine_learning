{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from scratch\n",
    "\n",
    "Today, you'll learn how to code and train a neural network from scratch using just `numpy`.\n",
    "\n",
    "Let's start with a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=50, noise=0.1, random_state=42)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1) # make y a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* How many observations does the data have?\n",
    "* How many input features are there?\n",
    "* Why would a simple Logistic Regression (LogReg) model perform poorly \n",
    "* How many model parameters (weights) does a LogReg model have for this task?\n",
    "* How could one make a logistic regression perform better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fitting a linear model\n",
    "\n",
    "we need:\n",
    "\n",
    "1. A prediction function that maps the input `X` to the output `y`: $\\hat{y} = F(X;w)$\n",
    "2. A loss function that evaluates the goodness of fit: $L(y, \\hat{y})$\n",
    "3. Training data that is used to find the weights `w` that minimize the loss function.\n",
    "4. Separate validation data that is used to assess the model's performance on unseen data.\n",
    "5. The Gradient Descent Algorithm:\n",
    "\n",
    "    $$\n",
    "    w_{new} = w_{old} - LR \\cdot \\nabla_{L_{Loss}}(w)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a Log Reg model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X) = sigmoid(w_01 + w_1X_1 + w_2X_2) = sigmoid(Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of ones to the input data\n",
    "\n",
    "def add_bias(X):\n",
    "    return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "X = add_bias(X)\n",
    "\n",
    "# a model parameter for each column\n",
    "assert X.shape[1] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize three random weights from the normal distribution\n",
    "w = np.random.normal(size=3)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the linear combination between the input and the weights with a DOT product\n",
    "# X[:,0]*w[0] + X[:,1]*w[1] + X[:,2]*w[2]\n",
    "X.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sigmoid non linear transformation\n",
    "def sigmoid(a):\n",
    "    return ...\n",
    "    \n",
    "a = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
    "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
    "assert np.all(sigmoid(a).round(2) == expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the log loss (aka binary crossentropy)\n",
    "def log_loss(y, y_pred):\n",
    "    return ...\n",
    "\n",
    "a = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "b = np.array([0.01, 0.99, 0.01, 0.99])\n",
    "expected = np.array([0.01, 4.61, 4.61, 0.01])\n",
    "assert np.all(log_loss(a, b).round(2) == expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions of a log reg model\n",
    "y_pred = sigmoid(X.dot(w))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Network\n",
    "\n",
    "We build a Neural Net with \n",
    "\n",
    "- one hidden layer that contains 3 \"neurons\"/ units\n",
    "- one output layer with 1 unit\n",
    "- a `sigmoid` **activation** function\n",
    "\n",
    "$$\n",
    "\\hat{y} = F(X; w_h, w_o) = act(act(Xw_h)w_o)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how can we calculate the output of several LogReg models at the same time? \n",
    "# this is the first layer of a neural net!\n",
    "\n",
    "weights = []\n",
    "\n",
    "weights.append(np.random.normal(size=(3, 2))) # 3 model parameters for each units/neurons\n",
    "\n",
    "# output layer: last layer of the network\n",
    "# binary classification = one unit\n",
    "weights.append(np.random.normal(size=(3, 1)))\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the first layer: stacked output of three logistic regressions \n",
    "X_hidden = sigmoid(X.dot(weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the output of the first hidden layer into a second layer! this is an ordinary logistic regression.\n",
    "\n",
    "# add a bias\n",
    "X_hidden_with_bias = add_bias(X_hidden)\n",
    "\n",
    "# calculate the final output of the network\n",
    "ypred = sigmoid(X_hidden_with_bias.dot(weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feed forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights):\n",
    "    \"\"\"\n",
    "    1. Calculate the dot product of X\n",
    "       and the weights of the first layer.\n",
    "\n",
    "    2. Apply the sigmoid function on the result.\n",
    "\n",
    "    3. Append an extra column of ones to the result (i.e. the bias).\n",
    "\n",
    "    4. Calculate the dot product of the previous step\n",
    "       with the weights of the second (i.e. outer) layer.\n",
    "\n",
    "    5. Apply the sigmoid function on the result.\n",
    "\n",
    "    6. Return all intermediate results (i.e. anything that is outputted\n",
    "       by an activation function).\n",
    "    \"\"\" \n",
    "    ...\n",
    "\n",
    "    return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some random weights\n",
    "\n",
    "weights = [\n",
    "    np.random.normal(size=(3, 2)),\n",
    "    np.random.normal(size=(3, 1))\n",
    "]\n",
    "\n",
    "# testing \n",
    "\n",
    "out1, out2 = feed_forward(X, weights)\n",
    "\n",
    "assert out1.shape == (50, 2)\n",
    "assert out2.shape == (50, 1)\n",
    "\n",
    "Xref = np.array([[1.0, 2.0, 1.0]])\n",
    "\n",
    "out1, out2 = feed_forward(Xref, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Backpropagation algorithm\n",
    "\n",
    "http://krspiced.pythonanywhere.com/chapters/project_deep_learning/neural_networks/backpropagation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(X):\n",
    "   \"\"\"derivative of sigmoid with respect to X\"\"\"\n",
    "   return sigmoid(X) * (1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_deriv(ytrue, ypred):\n",
    "    loss_deriv = -(ytrue/ypred - ((1-ytrue)/(1-ypred)))  #transcribe the formula above\n",
    "    return loss_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(weights,\n",
    "             output1,\n",
    "             output2,\n",
    "             ytrue,\n",
    "             X_input,\n",
    "             LR):\n",
    "\n",
    "    wH = weights[0]\n",
    "    wO = weights[1]\n",
    "\n",
    "    '''EQUATION A:'''\n",
    "    loss_gradient = log_loss_deriv(ytrue , output2)\n",
    "\n",
    "    '''EQUATION B:'''\n",
    "    # don't forget the bias!\n",
    "    hidden_out_with_bias = add_bias(output1)\n",
    "    # derivative of the sigmoid function with respect to the\n",
    "    # hidden output * weights\n",
    "    sig_deriv_1 = sigmoid_deriv(hidden_out_with_bias.dot(wO))\n",
    "\n",
    "    y_grad = sig_deriv_1 * loss_gradient\n",
    "\n",
    "    '''EQUATION C:'''\n",
    "    delta_wo = -np.dot( y_grad.T, hidden_out_with_bias ) * LR\n",
    "\n",
    "    #and finally, old weights + delta weights -> new weights!\n",
    "    wO_new = wO + delta_wo.T\n",
    "\n",
    "    '''EQUATION D:'''\n",
    "    sig_deriv_2 = sigmoid_deriv( X_input.dot(wH) )\n",
    "    #exclude the bias (last column) of the outer weights,\n",
    "    #since it is not backpropagated!\n",
    "    H_grad = sig_deriv_2  * np.dot(y_grad , wO[:-1].T)\n",
    "\n",
    "    '''EQUATION E:'''\n",
    "    delta_wH = -np.dot(H_grad.T, X_input ) * LR\n",
    "\n",
    "    #old weights + delta weights -> new weights!\n",
    "    wH_new = wH + delta_wH.T\n",
    "\n",
    "    # new hidden weights, new output weights\n",
    "    return wH_new, wO_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X = add_bias(X)\n",
    "y = y.reshape(-1, 1)\n",
    "weights = [\n",
    "   np.random.normal(size=(3, 4)),\n",
    "   np.random.normal(size=(5, 1))\n",
    "]\n",
    "\n",
    "# train\n",
    "history = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Visualizing the decision boundary\n",
    "\n",
    "The decision boundary looks random as we have not trained our neural network yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of values\n",
    "x = np.linspace(-3, 3, 200)\n",
    "X_vis = np.array([(x1, x2) for x1 in x for x2 in x])\n",
    "# add the bias column\n",
    "X_vis = add_bias(X_vis)\n",
    "\n",
    "# calculate the (random) predictions\n",
    "_, y_pred = feed_forward(X_vis, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the predictions for visualization\n",
    "Z = y_pred.reshape((len(x), len(x)), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a contour plot\n",
    "fig,ax=plt.subplots(1,1)\n",
    "cp = ax.contourf(x, x, Z, alpha=0.8, cmap='coolwarm')\n",
    "ax.contour(x, x, Z, levels=[0.5])\n",
    "fig.colorbar(cp) # Add a colorbar to a plot\n",
    "\n",
    "# draw the original data\n",
    "ax.scatter(X[:,0], X[:,1], c=y.flatten(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
